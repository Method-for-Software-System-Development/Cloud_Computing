{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcOdiuUsiCNcO3kv3IvHo6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Method-for-Software-System-Development/Cloud_Computing/blob/develop/logic/chatbot_controller.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "OptiBot Chatbot Controller\n",
        "\n",
        "This module provides the main backend logic for the OptiLine virtual assistant chatbot (\"OptiBot\").\n",
        "The chatbot can answer user questions about the OptiLine system, leveraging both generative AI (Gemini) and live data from Firebase when needed.\n",
        "\n",
        "Functions:\n",
        "    - ask_optibot(question: str) -> str\n",
        "        Main entry point. Receives a user question (string) and returns a single English answer as a string.\n",
        "        Answers can include both real-time data from the system and general knowledge.\n",
        "\n",
        "Dependencies:\n",
        "    - google-generativeai (for Gemini API)\n",
        "    - importnb (for loading other project notebooks as modules)\n",
        "    - Firebase helper modules (existing: FireBase, user_controller, etc.)\n",
        "    - Colab secrets (for securely storing Gemini API key)\n",
        "\n",
        "Note:\n",
        "    This module is backend-only; it does not include any frontend or UI logic.\n",
        "    All code comments and docstrings are in English for documentation purposes.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "7hy9NPq-cCoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ─── Imports ───\n",
        "\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "import time\n",
        "\n",
        "# Import project modules using importnb\n",
        "from importnb import Notebook\n",
        "\n",
        "# Load Firebase and helper modules as notebooks\n",
        "with Notebook():\n",
        "    import FireBase as fb           # Firebase data access functions\n",
        "    import user_controller as uc    # User management and leaderboard functions\n",
        "    import mqqt_sim_indoor as indoor    # Indoor sensor simulator\n",
        "    import mqqt_sim_outdoor as outdoor  # Outdoor sensor simulator\n",
        "\n",
        "# ─── Constants ───\n",
        "\n",
        "# The base context provided to the Gemini model for every answer\n",
        "OPTI_BOT_CONTEXT = \"\"\"\n",
        "You are OptiBot – a virtual assistant for the OptiLine system.\n",
        "\n",
        "OptiLine is a smart cloud-based dashboard for real-time monitoring, fault detection, optimization, and performance tracking in autonomous production lines.\n",
        "\n",
        "Key features include:\n",
        "- Real-time sensor data (temperature, speed, accuracy, energy)\n",
        "- Alerts within 1 second for anomalies\n",
        "- Gamified optimization challenges (Optimization Race)\n",
        "- Performance score and team leaderboard\n",
        "- Intelligent fault response with scoring\n",
        "\n",
        "Answer all questions as if you are part of the OptiLine platform.\n",
        "\"\"\"\n",
        "\n",
        "# The Gemini API key is securely retrieved from Colab secrets\n",
        "GEMINI_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n"
      ],
      "metadata": {
        "id": "ww-rJ9kRcQGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRh9jhp6Z_YJ"
      },
      "outputs": [],
      "source": [
        "def ask_optibot(message: str, chat_history: list) -> tuple:\n",
        "    \"\"\"\n",
        "    Main entry point for OptiBot Q&A.\n",
        "\n",
        "    Args:\n",
        "        message (str): The latest user message (question) as a string.\n",
        "        chat_history (list): A list of previous chat messages.\n",
        "\n",
        "    Returns:\n",
        "        tuple:\n",
        "            - Empty string (to clear the input textbox in Gradio)\n",
        "            - Updated chat_history (list), including the new user message and the generated bot response.\n",
        "\n",
        "    Logic flow:\n",
        "        1. Analyze the incoming message to determine if it requests live sensor data (indoor/outdoor)\n",
        "           or general system data (leaderboard, faults, etc.).\n",
        "        2. If live data is needed, fetch relevant data from the proper module (mqqt_sim_indoor/outdoor).\n",
        "        3. Construct a prompt for the Gemini model that includes:\n",
        "           - System context\n",
        "           - (Optionally) real-time data retrieved from the system\n",
        "           - The user's original question\n",
        "        4. Call the Gemini API to generate an English response.\n",
        "        5. Append the user message and the bot response to the chat history.\n",
        "        6. Return an empty string (to clear the Gradio input box) and the updated chat history.\n",
        "    \"\"\"\n",
        "\n",
        "    # Add the new user message to chat history\n",
        "    chat_history = chat_history + [[message, None]]\n",
        "    message_lower = message.lower()\n",
        "    live_data_note = \"\"\n",
        "\n",
        "    # --- Step 1: Pattern matching for live data intent ---\n",
        "\n",
        "    # Indoor temperature\n",
        "    if \"indoor temperature\" in message_lower:\n",
        "        try:\n",
        "            indoor_data = next(indoor.get_live_data_stream(mode=\"simulation\"))\n",
        "            live_data_note = f\"Current indoor temperature: {indoor_data['Temperature']} °C.\"\n",
        "        except Exception as e:\n",
        "            live_data_note = f\"Unable to retrieve indoor temperature data: {e}\"\n",
        "\n",
        "    # Outdoor temperature\n",
        "    elif \"outdoor temperature\" in message_lower:\n",
        "        try:\n",
        "            outdoor_data = next(outdoor.get_live_data_stream(mode=\"simulation\"))\n",
        "            live_data_note = f\"Current outdoor temperature: {outdoor_data['Temperature']} °C.\"\n",
        "        except Exception as e:\n",
        "            live_data_note = f\"Unable to retrieve outdoor temperature data: {e}\"\n",
        "\n",
        "    # General temperature (prefers outdoor if not specified)\n",
        "    elif \"temperature\" in message_lower:\n",
        "        try:\n",
        "            outdoor_data = next(outdoor.get_live_data_stream(mode=\"simulation\"))\n",
        "            live_data_note = f\"Current outdoor temperature: {outdoor_data['Temperature']} °C.\"\n",
        "        except Exception as e:\n",
        "            live_data_note = f\"Unable to retrieve temperature data: {e}\"\n",
        "\n",
        "    # Leaderboard\n",
        "    elif \"score\" in message_lower or \"leaderboard\" in message_lower:\n",
        "        top_users, user_rank = uc.get_leaderboard(\"\")\n",
        "        if top_users:\n",
        "            table = \", \".join([f\"{name} ({score})\" for _, name, score in top_users])\n",
        "            live_data_note = f\"Leaderboard: {table}.\"\n",
        "        else:\n",
        "            live_data_note = \"Leaderboard data is currently unavailable.\"\n",
        "\n",
        "    # Faults\n",
        "    elif \"fault\" in message_lower or \"error\" in message_lower:\n",
        "        faults = getattr(fc.fb, \"get_active_faults\", lambda: {})()\n",
        "        if faults:\n",
        "            num_faults = len(faults)\n",
        "            live_data_note = f\"There are currently {num_faults} active faults in the system.\"\n",
        "        else:\n",
        "            live_data_note = \"No active faults found at the moment.\"\n",
        "\n",
        "    # Add more sensor patterns as needed\n",
        "\n",
        "    # --- Step 2: Build prompt for Gemini ---\n",
        "    prompt = OPTI_BOT_CONTEXT.strip() + \"\\n\"\n",
        "    if live_data_note:\n",
        "        prompt += f\"\\nRelevant live system data:\\n{live_data_note}\\n\"\n",
        "    prompt += f\"\\nUser question: {message}\\n\"\n",
        "\n",
        "    # --- Step 3: Call Gemini API ---\n",
        "    try:\n",
        "        if not GEMINI_API_KEY:\n",
        "            bot_response = \"Error: Gemini API key is missing. Please contact your administrator.\"\n",
        "        else:\n",
        "            genai.configure(api_key=GEMINI_API_KEY)\n",
        "            model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "            response = model.generate_content(prompt)\n",
        "            bot_response = response.text.strip() if hasattr(response, \"text\") else str(response)\n",
        "    except Exception as ex:\n",
        "        bot_response = f\"Sorry, I couldn't process your question due to a system error: {ex}\"\n",
        "\n",
        "    # --- Step 4: Add bot response to chat history and return ---\n",
        "    chat_history[-1][1] = bot_response\n",
        "    return \"\", chat_history\n"
      ]
    }
  ]
}