{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+z2PI2cmngGKkes5YynxQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Method-for-Software-System-Development/Cloud_Computing/blob/develop/logic/search_index.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHxMC3X59fzQ"
      },
      "outputs": [],
      "source": [
        "DBLink = \"https://cloud-project-5adfc-default-rtdb.europe-west1.firebasedatabase.app/\"\n",
        "url = 'https://mqtt.org/'\n",
        "\n",
        "import requests\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.stem import PorterStemmer\n",
        "from firebase import firebase\n",
        "import matplotlib.pyplot as plt\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "# Get the data of the selected url\n",
        "def fetch_page(url):\n",
        " response = requests.get(url)\n",
        " if response.status_code == 200:\n",
        "  soup = BeautifulSoup(response.text, 'html.parser')\n",
        "  return soup\n",
        " else:\n",
        "  return None\n",
        "\n",
        "# Create index of the loaded page\n",
        "def index_words(soup):\n",
        "  index = {}\n",
        "  words = re.findall(r'(?!\\d)\\w+', soup.get_text())\n",
        "  for word in words:\n",
        "    word = word.lower()\n",
        "    if word in index:\n",
        "      index[word] += 1\n",
        "    else:\n",
        "      index[word] = 1\n",
        "  return index\n",
        "\n",
        "# Remove the stop words from the index\n",
        "def remove_stop_words(index):\n",
        "  stop_words = {'a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves'}\n",
        "  for stop_word in stop_words:\n",
        "    if stop_word in index:\n",
        "      del index[stop_word]\n",
        "  return index\n",
        "\n",
        "# Apply stemming to the index\n",
        "def apply_stemming(index):\n",
        "  stemmer = PorterStemmer()\n",
        "  stemmed_index = {}\n",
        "  for word, count in index.items():\n",
        "    stemmed_word = stemmer.stem(word)\n",
        "    if stemmed_word in stemmed_index:\n",
        "      stemmed_index[stemmed_word] += count\n",
        "    else:\n",
        "      stemmed_index[stemmed_word] = count\n",
        "  return stemmed_index\n",
        "\n",
        "\n",
        "def remove_infrequent_words(index, threshold):\n",
        "  for word, count in list(index.items()):\n",
        "    if count < threshold:\n",
        "      del index[word]\n",
        "  return index\n",
        "\n",
        "# Run the index creation of selected url\n",
        "def create_index(url):    # rename to index creaation\n",
        "  soup = fetch_page(url)\n",
        "  if soup is None:\n",
        "     return None\n",
        "  index = index_words(soup)\n",
        "  index = remove_stop_words(index)\n",
        "  index = apply_stemming(index)\n",
        "  index = remove_infrequent_words(index, 2)\n",
        "  return index\n",
        "\n",
        "def get_sub_urls(start_url):\n",
        "    \"\"\"\n",
        "    Fetches all sub-URLs from a given URL site.\n",
        "\n",
        "    Args:\n",
        "        url (str): The base URL to crawl.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of sub-URLs in the site.\n",
        "    \"\"\"\n",
        "    url_query = [start_url]\n",
        "    sub_urls = [start_url]\n",
        "    while url_query != []:\n",
        "      # get first url\n",
        "      url = url_query[0]\n",
        "      url_query.remove(url)\n",
        "\n",
        "      # run scrape on the current page\n",
        "      response = requests.get(url)      #ToDo: need to run in depth\n",
        "      response.raise_for_status()  # Raise an exception for bad responses\n",
        "\n",
        "      soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "      for link in soup.find_all('a', href=True):\n",
        "          href = link['href']\n",
        "          absolute_url = urljoin(url, href)  # Make URL absolute\n",
        "\n",
        "          if absolute_url.startswith(url) and absolute_url != url and absolute_url not in sub_urls:\n",
        "              sub_urls.append(absolute_url)\n",
        "\n",
        "              if absolute_url not in url_query:\n",
        "                  url_query.append(absolute_url)\n",
        "\n",
        "    return sub_urls\n",
        "\n",
        "# This section will run on every sub site and create the index\n",
        "\n",
        "# get the sub url list\n",
        "site_urls = get_sub_urls(url)\n",
        "\n",
        "site_index = {}\n",
        "for sub_url in site_urls:\n",
        "  idx = create_index(sub_url)\n",
        "\n",
        "  # marge idx to site_index\n",
        "  for word in idx:\n",
        "    if word in site_index:\n",
        "      site_index[word].append((sub_url, idx[word]))\n",
        "    else:\n",
        "      site_index[word] = [(sub_url, idx[word])]\n",
        "\n",
        "# Save the index to FireBase\n",
        "\n",
        "# Initialize Firebase connection\n",
        "FBconn = firebase.FirebaseApplication(DBLink, None)\n",
        "\n",
        "# Upload the site_index to Firebase\n",
        "FBconn.put('/', 'site_index', site_index)\n",
        "\n",
        "from difflib import get_close_matches\n",
        "\n",
        "def read_index_from_firebase():\n",
        "    \"\"\"Reads the site index from the Firebase database.\"\"\"\n",
        "    FBconn = firebase.FirebaseApplication(DBLink, None)\n",
        "    site_index = FBconn.get('/', 'site_index')\n",
        "    return site_index if site_index else {}\n",
        "\n",
        "def search_words(query):\n",
        "    \"\"\"\n",
        "    Searches for words in the index and returns the URLs ordered by count.\n",
        "    Attempts to fix typos and suggest unstemmed words from a dictionary.\n",
        "    Formats output as HTML with clickable links.\n",
        "    \"\"\"\n",
        "    site_index = read_index_from_firebase()\n",
        "    if not site_index:\n",
        "        # Return a simple error message formatted as HTML\n",
        "        return \"<p>Error: Could not read index from Firebase.</p>\"\n",
        "\n",
        "    stemmer = PorterStemmer()\n",
        "    query_words = re.findall(r'\\w+', query.lower())\n",
        "    output_html = \"\" # Use an HTML string for output\n",
        "\n",
        "    for word in query_words:\n",
        "        stemmed_word = stemmer.stem(word)\n",
        "\n",
        "        if stemmed_word in site_index:\n",
        "            output_html += f\"<h2>Results for '{word}':</h2>\"\n",
        "            # Sort URLs by count in descending order\n",
        "            sorted_urls = sorted(site_index[stemmed_word], key=lambda item: item[1], reverse=True)\n",
        "            output_html += \"<ul>\" # Use an unordered list for the links\n",
        "            for url, count in sorted_urls:\n",
        "                # Format the URL as an HTML link\n",
        "                output_html += f\"<li><a href='{url}' target='_blank'>{url}</a> (Count: {count})</li>\"\n",
        "            output_html += \"</ul>\"\n",
        "            output_html += \"<br>\" # Add a break after each word's results\n",
        "        else:\n",
        "            # Try to fix typo (search for the steammed word)\n",
        "            # ToDo?: try to create the currect word\n",
        "            all_words = list(site_index.keys())\n",
        "            matches = get_close_matches(stemmed_word, all_words, n=1, cutoff=0.8)\n",
        "            if matches:\n",
        "                corrected_stemmed_word = matches[0]\n",
        "\n",
        "                output_html += f\"<p>Did you mean '{corrected_stemmed_word}'?</p>\"\n",
        "                output_html += f\"<h2>Results for '{corrected_stemmed_word}':</h2>\"\n",
        "\n",
        "                # Sort URLs by count in descending order\n",
        "                sorted_urls = sorted(site_index[corrected_stemmed_word], key=lambda item: item[1], reverse=True)\n",
        "                output_html += \"<ul>\" # Use an unordered list for the links\n",
        "                for url, count in sorted_urls:\n",
        "                     # Format the URL as an HTML link\n",
        "                     output_html += f\"<li><a href='{url}' target='_blank'>{url}</a> (Count: {count})</li>\"\n",
        "                output_html += \"</ul>\"\n",
        "                output_html += \"<br>\"\n",
        "            else:\n",
        "                output_html += f\"<p>No results found for '{word}' and no close matches found.</p><br>\"\n",
        "\n",
        "    if not output_html: # Check if any output was generated\n",
        "        return \"<p>No results found for your query.</p>\"\n",
        "\n",
        "    return output_html"
      ]
    }
  ]
}