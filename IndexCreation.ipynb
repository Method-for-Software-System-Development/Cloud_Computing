{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3LpmLl5egZ1sBSZha4xvB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Method-for-Software-System-Development/Cloud_Computing/blob/develop/IndexCreation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create index and save it to the Firebase\n",
        "\n",
        "## Steps\n",
        "1. install the required packages\n",
        "2. edit the DBLink if needed\n",
        "3. Run every block\n",
        "4. The Index will be in the Firebase db\n",
        "\n",
        "The index will have about 562 words after removing:\n",
        "* stop words\n",
        "* words that start with number\n",
        "* words that appere only once\n",
        "\n"
      ],
      "metadata": {
        "id": "-15v2OtBvlOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4\n",
        "!pip install firebase"
      ],
      "metadata": {
        "id": "-YbDw-68vhfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYJrrcZLvUPj"
      },
      "outputs": [],
      "source": [
        "DBLink = \"https://optiline-kakado-default-rtdb.europe-west1.firebasedatabase.app/\"\n",
        "\n",
        "url = 'https://mqtt.org/'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.stem import PorterStemmer\n",
        "from firebase import firebase\n",
        "import matplotlib.pyplot as plt\n",
        "from urllib.parse import urljoin\n",
        "from typing_extensions import final"
      ],
      "metadata": {
        "id": "58Lt1NQJw0WI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the data of the selected url\n",
        "def fetch_page(url):\n",
        " response = requests.get(url)\n",
        " if response.status_code == 200:\n",
        "  soup = BeautifulSoup(response.text, 'html.parser')\n",
        "  return soup\n",
        " else:\n",
        "  return None\n",
        "\n",
        "# Create index of the loaded page\n",
        "def index_words(soup):\n",
        "  index = {}\n",
        "  words = re.findall(r'(?!\\d)\\w+', soup.get_text())\n",
        "  for word in words:\n",
        "    word = word.lower()\n",
        "    if word in index:\n",
        "      index[word] += 1\n",
        "    else:\n",
        "      index[word] = 1\n",
        "  return index\n",
        "\n",
        "# Remove the stop words from the index\n",
        "def remove_stop_words(index):\n",
        "  stop_words = {'a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves'}\n",
        "  for stop_word in stop_words:\n",
        "    if stop_word in index:\n",
        "      del index[stop_word]\n",
        "  return index\n",
        "\n",
        "# Apply stemming to the index\n",
        "def apply_stemming(index):\n",
        "  stemmer = PorterStemmer()\n",
        "  stemmed_index = {}\n",
        "  for word, count in index.items():\n",
        "    stemmed_word = stemmer.stem(word)\n",
        "    if stemmed_word in stemmed_index:\n",
        "      stemmed_index[stemmed_word] += count\n",
        "    else:\n",
        "      stemmed_index[stemmed_word] = count\n",
        "  return stemmed_index\n",
        "\n",
        "\n",
        "def remove_infrequent_words(index, threshold):\n",
        "  for word, count in list(index.items()):\n",
        "    if count < threshold:\n",
        "      del index[word]\n",
        "  return index\n",
        "\n",
        "# Run the index creation of selected url\n",
        "def create_index(url):    # rename to index creaation\n",
        "  soup = fetch_page(url)\n",
        "  if soup is None:\n",
        "     return None\n",
        "  index = index_words(soup)\n",
        "  index = remove_stop_words(index)\n",
        "  index = apply_stemming(index)\n",
        "  index = remove_infrequent_words(index, 2)\n",
        "  return index\n",
        "\n",
        "  def get_sub_urls(start_url):\n",
        "    \"\"\"\n",
        "    Fetches all sub-URLs from a given URL site.\n",
        "\n",
        "    Args:\n",
        "        url (str): The base URL to crawl.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of sub-URLs in the site.\n",
        "    \"\"\"\n",
        "    url_query = [start_url]\n",
        "    sub_urls = [start_url]\n",
        "    while url_query != []:\n",
        "      # get first url\n",
        "      url = url_query[0]\n",
        "      url_query.remove(url)\n",
        "\n",
        "      # run scrape on the current page\n",
        "      response = requests.get(url)      #ToDo: need to run in depth\n",
        "      response.raise_for_status()  # Raise an exception for bad responses\n",
        "\n",
        "      soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "      for link in soup.find_all('a', href=True):\n",
        "          href = link['href']\n",
        "          absolute_url = urljoin(url, href)  # Make URL absolute\n",
        "\n",
        "          if absolute_url.startswith(url) and absolute_url != url and absolute_url not in sub_urls:\n",
        "              sub_urls.append(absolute_url)\n",
        "\n",
        "              if absolute_url not in url_query:\n",
        "                  url_query.append(absolute_url)\n",
        "\n",
        "    return sub_urls\n"
      ],
      "metadata": {
        "id": "DDtfKnokw3mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This section will run on every sub site and create the index\n",
        "\n",
        "# get the sub url list\n",
        "site_urls = get_sub_urls(url)\n",
        "\n",
        "site_index = {}\n",
        "for sub_url in site_urls:\n",
        "  idx = create_index(sub_url)\n",
        "\n",
        "  # marge idx to site_index\n",
        "  for word in idx:\n",
        "    if word in site_index:\n",
        "      site_index[word].append((sub_url, idx[word]))\n",
        "    else:\n",
        "      site_index[word] = [(sub_url, idx[word])]\n",
        "\n",
        "# Initialize Firebase connection\n",
        "FBconn = firebase.FirebaseApplication(DBLink, None)\n",
        "\n",
        "# convert the index to firebase format\n",
        "final_index = {}\n",
        "for key, value in site_index.items():\n",
        "  item = {\n",
        "      \"term\": key,\n",
        "      \"DocIDs\": [x[0] for x in value],\n",
        "      \"DocCounts\": [x[1] for x in value]\n",
        "  }\n",
        "  final_index[key] = item\n",
        "\n",
        "# Upload the site_index to Firebase\n",
        "FBconn.put('/' ,'site_index',final_index)"
      ],
      "metadata": {
        "id": "r9uSTP2CxB1R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}