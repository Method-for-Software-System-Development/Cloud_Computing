{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOalhyuyt3hff0wfLKjMk44",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Method-for-Software-System-Development/Cloud_Computing/blob/develop/login/chatbot_controller.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "OptiBot Chatbot Controller\n",
        "\n",
        "This module provides the main backend logic for the OptiLine virtual assistant chatbot (\"OptiBot\").\n",
        "The chatbot can answer user questions about the OptiLine system, leveraging both generative AI (Gemini) and live data from Firebase when needed.\n",
        "\n",
        "Functions:\n",
        "    - ask_optibot(question: str) -> str\n",
        "        Main entry point. Receives a user question (string) and returns a single English answer as a string.\n",
        "        Answers can include both real-time data from the system and general knowledge.\n",
        "\n",
        "Dependencies:\n",
        "    - google-generativeai (for Gemini API)\n",
        "    - importnb (for loading other project notebooks as modules)\n",
        "    - Firebase helper modules (existing: FireBase, user_controller, etc.)\n",
        "    - Colab secrets (for securely storing Gemini API key)\n",
        "\n",
        "Note:\n",
        "    This module is backend-only; it does not include any frontend or UI logic.\n",
        "    All code comments and docstrings are in English for documentation purposes.\n",
        "\"\"\"\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "from importnb import Notebook\n",
        "\n",
        "# Load helpers from other modules\n",
        "with Notebook():\n",
        "    import FireBase as fb\n",
        "    import sensors_stats as stats\n",
        "    import user_controller as uc\n",
        "    import mqqt_sim_outdoor as outdoor\n",
        "    import mqqt_sim_indoor as indoor"
      ],
      "metadata": {
        "id": "7hy9NPq-cCoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_optibot(message: str, chat_history: list) -> tuple:\n",
        "    \"\"\"\n",
        "    Main entry point for OptiBot Q&A.\n",
        "\n",
        "    Args:\n",
        "        message (str): The latest user message (question) as a string.\n",
        "        chat_history (list): A list of previous chat messages, each as a dictionary:\n",
        "            {\"role\": \"user\" | \"assistant\", \"content\": str}\n",
        "\n",
        "    Returns:\n",
        "        tuple:\n",
        "            - Empty string (to clear the input textbox in Gradio)\n",
        "            - Updated chat_history (list), including the new user message and the generated bot response.\n",
        "\n",
        "    Logic flow:\n",
        "        1. Analyze the incoming message to determine if it requests live sensor data (indoor/outdoor)\n",
        "           or general system data (leaderboard, faults, etc.).\n",
        "        2. If live data is needed, fetch relevant data from the proper module (mqqt_sim_indoor/outdoor).\n",
        "        3. Construct a prompt for the Gemini model that includes:\n",
        "           - System context\n",
        "           - (Optionally) real-time data retrieved from the system\n",
        "           - The user's original question\n",
        "        4. Call the Gemini API to generate an English response.\n",
        "        5. Append the user message and the bot response to the chat history.\n",
        "        6. Return an empty string (to clear the Gradio input box) and the updated chat history.\n",
        "    \"\"\"\n",
        "\n",
        "    if chat_history is None:\n",
        "        chat_history = []\n",
        "\n",
        "    # Step 1: Add the new user message to chat history (dict format)\n",
        "    chat_history.append({\"role\": \"user\", \"content\": message})\n",
        "    message_lower = message.lower()\n",
        "    live_data_note = \"\"\n",
        "\n",
        "    # Step 2: Pattern matching for live data intent\n",
        "\n",
        "    # Indoor temperature\n",
        "    if \"indoor temperature\" in message_lower:\n",
        "        try:\n",
        "            indoor_data = next(indoor.get_live_data_stream(mode=\"simulation\"))\n",
        "            live_data_note = f\"Current indoor temperature: {indoor_data['Temperature']} °C.\"\n",
        "        except Exception as e:\n",
        "            live_data_note = f\"Unable to retrieve indoor temperature data: {e}\"\n",
        "\n",
        "    # Outdoor temperature\n",
        "    elif \"outdoor temperature\" in message_lower:\n",
        "        try:\n",
        "            outdoor_data = next(outdoor.get_live_data_stream(mode=\"simulation\"))\n",
        "            live_data_note = f\"Current outdoor temperature: {outdoor_data['Temperature']} °C.\"\n",
        "        except Exception as e:\n",
        "            live_data_note = f\"Unable to retrieve outdoor temperature data: {e}\"\n",
        "\n",
        "    # General temperature (prefers outdoor if not specified)\n",
        "    elif \"temperature\" in message_lower:\n",
        "        try:\n",
        "            outdoor_data = next(outdoor.get_live_data_stream(mode=\"simulation\"))\n",
        "            live_data_note = f\"Current outdoor temperature: {outdoor_data['Temperature']} °C.\"\n",
        "        except Exception as e:\n",
        "            live_data_note = f\"Unable to retrieve temperature data: {e}\"\n",
        "\n",
        "    # Leaderboard\n",
        "    elif \"score\" in message_lower or \"leaderboard\" in message_lower:\n",
        "        top_users, user_rank = uc.get_leaderboard(\"\")\n",
        "        if top_users:\n",
        "            table = \", \".join([f\"{name} ({score})\" for _, name, score in top_users])\n",
        "            live_data_note = f\"Leaderboard: {table}.\"\n",
        "        else:\n",
        "            live_data_note = \"Leaderboard data is currently unavailable.\"\n",
        "\n",
        "    # Faults\n",
        "    elif \"fault\" in message_lower or \"error\" in message_lower:\n",
        "        faults = getattr(fc.fb, \"get_active_faults\", lambda: {})()\n",
        "        if faults:\n",
        "            num_faults = len(faults)\n",
        "            live_data_note = f\"There are currently {num_faults} active faults in the system.\"\n",
        "        else:\n",
        "            live_data_note = \"No active faults found at the moment.\"\n",
        "\n",
        "    # Step 3: Build prompt for Gemini\n",
        "    prompt = OPTI_BOT_CONTEXT.strip() + \"\\n\"\n",
        "    if live_data_note:\n",
        "        prompt += f\"\\nRelevant live system data:\\n{live_data_note}\\n\"\n",
        "    prompt += f\"\\nUser question: {message}\\n\"\n",
        "\n",
        "    # Step 4: Call Gemini API\n",
        "    try:\n",
        "        if not GEMINI_API_KEY:\n",
        "            bot_response = \"Error: Gemini API key is missing. Please contact your administrator.\"\n",
        "        else:\n",
        "            genai.configure(api_key=GEMINI_API_KEY)\n",
        "            model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "            response = model.generate_content(prompt)\n",
        "            bot_response = response.text.strip() if hasattr(response, \"text\") else str(response)\n",
        "    except Exception as ex:\n",
        "        bot_response = f\"Sorry, I couldn't process your question due to a system error: {ex}\"\n",
        "\n",
        "    # Step 5: Add bot response to chat history (dict format)\n",
        "    chat_history.append({\"role\": \"assistant\", \"content\": bot_response})\n",
        "\n",
        "    # Step 6: Return updated chat history for Gradio\n",
        "    return \"\", chat_history\n"
      ],
      "metadata": {
        "id": "ww-rJ9kRcQGJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}